{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING PRACTICE - WEB SCRAPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # Librería para crear objetos tipo soup\n",
    "import urllib3 # Librería para descargar el contenido de una URL definida\n",
    "import numpy as np # Librería para manipular arreglos\n",
    "from sqlalchemy import create_engine # Librería para crear las conexiones con la base de datos Postgre\n",
    "from datetime import datetime\n",
    "from datetime import timedelta, time\n",
    "import pickle\n",
    "import requests\n",
    "import sys\n",
    "import re\n",
    "import random # Librería para generar números randómicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argumento que proviene del proceso de extracción de proxies\n",
    "proxies = {\n",
    "  'http': 'http://{}:{}'.format(proxy_list[proxy_index],port_list[proxy_index]),\n",
    "  'https': 'http://{}:{}'.format(proxy_list[proxy_index],port_list[proxy_index])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función download() para realizar la petición GET a una URL especificada\n",
    "def download():\n",
    "    # Aplicamos el módulo urllib3 para descargar una URL definida\n",
    "    num_retries = 5 # Número máximo de reintentos a efecuar en caso de error en la descarga\n",
    "    try:\n",
    "        http = urllib3.PoolManager()\n",
    "        response = http.request('GET', URL)\n",
    "        response.status\n",
    "        return response.data\n",
    "    \n",
    "    # En el caso de que exista un error colocamos una excepción para conocer el motivo\n",
    "    except urllib.URLError  as e: \n",
    "        print (\"Download error\", e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:  \n",
    "            ## Realizamos hasta 5 reintentos de la petición en caso de presentarse los errores más frecuentes\n",
    "            if hasattr(e, 'code') and 400 <= e.code < 600:  \n",
    "                return download(URL, num_retries-1) \n",
    "        return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAZON WEB SCRAPING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Se realiza la captura de los precios, descripciones y códigos únicos de los productos de Amazon (mascarillas):\\n\\nPara dicho objetivo se utilizan URLs de Amazon de diferentes países.\\n\\nEl resultado será un Dataset conteniendo los datos capturados de los siguientes campos:\\n\\nNo hay N/A detectados\\n\\n\\'asin\\',              tipo CHR          \"Amazon Standar Identification Number - Código único de identificación del producto\"\\n\\'description\\',       tipo CHR          \"Descripción del producto\"\\n\\'dateTime\\'           tipo timestamp    \"Fecha y hora en la que se realiza la captura de datos\"\\n\\'date\\'               tipo datetime     \"Fecha en la que se realiza la captura de datos\"\\n\\'country\\'            tipo CHR          \"País al que pertenece el producto \"\\n\\'currency\\'           tipo CHR          \"Moneda local en la que se expresa el precio del producto\"\\n\\'price\\'              tipo FLOAT        \"Precio del producto\"\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Se realiza la captura de los precios, descripciones y códigos únicos de los productos de Amazon (mascarillas):\n",
    "\n",
    "Para dicho objetivo se utilizan URLs de Amazon de diferentes países.\n",
    "\n",
    "El dataframe resultante con los datos en bruto es: \"dailyData\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon URL List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalle de URL de Amazon de cada país para la extracción diaria de datos\n",
    "urlSpain = 'https://www.amazon.es/s?k=MASCARILLAS+FFP2&__mk_es_ES=%C3%85M%C3%85%C5%BD%C3%95%C3%91&ref=nb_sb_noss'\n",
    "urlUSA = 'https://www.amazon.com/s?k=FFP2+mask&ref=nb_sb_noss_2'\n",
    "urlFrance = 'https://www.amazon.fr/s?k=masque+facial+FFP2&__mk_fr_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&ref=nb_sb_noss_2'\n",
    "urlUK = 'https://www.amazon.co.uk/s?k=FFP2+mask&ref=nb_sb_noss_2'\n",
    "urlGermany = 'https://www.amazon.de/s?k=Gesichtsmaske+FFP2&__mk_de_DE=%C3%85M%C3%85%C5%BD%C3%95%C3%91&ref=nb_sb_noss_2'\n",
    "urlItaly = 'https://www.amazon.it/s?k=maschera+FFP2&__mk_it_IT=%C3%85M%C3%85%C5%BD%C3%95%C3%91&ref=nb_sb_noss_2'\n",
    "urlNetherlands = 'https://www.amazon.nl/s?k=gezichtsmasker+ffp2&__mk_nl_NL=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=296W8O9PQ26GE&sprefix=gezichtsmasker+%2Caps%2C320&ref=nb_sb_ss_i_4_15'\n",
    "urlAustralia = 'https://www.amazon.com.au/s?k=FFP2+mask&ref=nb_sb_noss_2'\n",
    "\n",
    "# Lista con todas las URLs previamente definidas\n",
    "urlList = [urlSpain, urlUSA, urlFrance, urlUK, urlGermany, urlItaly, urlNetherlands, urlAustralia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "# Lista de los países que han sido seleccionados para efectuar scraping sobre sus páginas de Amazon\n",
    "countries = ['Spain', 'USA', 'France', 'UK', 'Germany', 'Italy', 'Netherlands', 'Australia']\n",
    "country = 0\n",
    "\n",
    "# Inicialización del dataframe final para almacenar todos los datos procesados\n",
    "dailyData = pd.DataFrame()\n",
    "\n",
    "# Bucle para scrapear datos por cada una de las URLs definidas previamente \n",
    "for i in urlList:   \n",
    "    URL = i\n",
    "    # Descarga del HTML con codificación UTF-8\n",
    "    maskInfo = download()\n",
    "    maskInfo = maskInfo.decode('utf-8')\n",
    "    # Conversión del HTML a objeto soup\n",
    "    soup = BeautifulSoup(maskInfo)\n",
    "    \n",
    "    # Inicialización de contadores para filas y columnas\n",
    "    row = 0\n",
    "    column = 0\n",
    "    \n",
    "    # Adquisición de la fecha actual en formato GMT\n",
    "    dateToProcess = datetime.utcnow()\n",
    "    \n",
    "    # Dataframe para almacenar la información de una sola URL\n",
    "    amazonData = pd.DataFrame({\n",
    "            'asin' : \"\",\n",
    "            'price': \"\",\n",
    "            'description': \"\",\n",
    "            'dateTime': dateToProcess,\n",
    "        }, index = { 'indice': 0 })\n",
    "\n",
    "    # Bucle para seleccionar todas las etiquetas ASIN, PRECIO y DESCRIPCIÓN del producto que se ofrece en Amazon\n",
    "    for div in soup.select('div[data-asin]'): # Se aplican selectores CSS al objeto soup\n",
    "        asin = div['data-asin']\n",
    "        column += 1\n",
    "        if div.select_one('.a-price'):\n",
    "            price = div.select_one('.a-price ').get_text('|',strip=True).split('|')[0]\n",
    "            column += 1\n",
    "        if div.select_one('.a-text-normal'):\n",
    "            description = div.select_one('.a-text-normal').text   \n",
    "        \n",
    "        # Dataframe para almacenar cada fila \n",
    "        temporalData = pd.DataFrame({\n",
    "            'asin' : asin,\n",
    "            'price': price,\n",
    "            'description': description,\n",
    "            'dateTime': dateToProcess,\n",
    "        }, index = { 'indice': 0 })\n",
    "        \n",
    "        # Agregamos la nueva fila al dataframe que creamos para cada URL\n",
    "        amazonData = amazonData.append(temporalData)\n",
    "        row += 1\n",
    "        column = 0\n",
    "    \n",
    "    # Aplicamos una primera normalización al conjunto de datos de la URL\n",
    "    amazonData = amazonData.reset_index(drop=True)\n",
    "    amazonData = amazonData.drop(amazonData.index[0])\n",
    "    amazonData = amazonData.reset_index(drop=True)\n",
    "    amazonData[\"date\"] = amazonData[\"dateTime\"].apply(lambda x: x.date())\n",
    "    amazonData = amazonData.drop_duplicates(subset =\"price\")\n",
    "    amazonData[\"country\"] = countries[country]\n",
    "    country += 1\n",
    "    \n",
    "    # Se agrega el dataframe con los datos de una URL al dataframe consolidado\n",
    "    dailyData = dailyData.append(amazonData) ## DATAFRAME CON LA INFORMACIÓN RECOPILADA DESDE LA WEB DE AMAZON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORONAVIRUS WEB SCRAPING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Vamos a captura la tabla de datos por país de afectación de la pandemia de Corona Virus en la web: \\n\\nhttps://www.worldometers.info/coronavirus/\\n\\nEl resultado será un Dataset conteniendo los datos capturados de los siguientes campos\\n\\nDatos de INT en unidades.\\n\\nNo hay N/A detectados. Los valores no detectos son CERO.\\n\\n\\'dateTime\\'           tipo timestamp    \"Fecha y hora en la que se realiza la captura de datos\"\\n\\'date\\'               tipo datetime     \"Fecha en la que se realiza la captura de datos\"\\n\\'country\\',           tipo CHR          \"País al que hacen referencia los datos, salvo un crucero\"\\n\\'total_cases\\'        tipo INT          \"Número total de casos que se han contabilizado en el país\"\\n\\'new_cases\\'          tipo INT          \"Número total de casos incrementados desde la info anterior en el país\"\\n\\'total_deaths\\'       tipo INT          \"Número total de fallidos en el país \"\\n\\'new_deaths\\'         tipo INT          \"Número total de fallacidos incrementados desde la info anterior en el país\"\\n\\'total_recovered\\'    tipo INT          \"Número total de recuperados en el país \"\\n\\'active_cases\\'       tipo INT          \"Número total de casos en tratamiento en el país \"\\n\\'critical_cases\\'     tipo INT          \"Número total en tratamiento en UCI en el país \"\\n\\'cases_per_million\\'  tipo INT          \"Ratio de casos por millón de habitantes en el país \"\\n\\'deaths_per_million\\' tipo INT          \"Ratio de muertes por millón de habitantes en el país \"\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Vamos a captura la tabla de datos por país de afectación de la pandemia de Corona Virus en la web: \n",
    "\n",
    "https://www.worldometers.info/coronavirus/\n",
    "\n",
    "El dataframe resultante con los datos en bruto es: \"datos_pandemia_df\" \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL for Coronavirus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = \"https://www.worldometers.info/coronavirus/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coronavirus Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque de preparación de estructuras de datos\n",
    "columnas = ['dateTime','country', 'total_cases','new_cases','total_deaths','new_deaths','total_recovered','active_cases','critical_cases','cases_per_million','deaths_per_million']\n",
    "\n",
    "# Creamos un DF vacio para contener los datos de esta captura de datos\n",
    "datos_pandemia_df = pd.DataFrame(columns=columnas)\n",
    "    \n",
    "# Imputamos la fecha y hora como constante a emplear en la captura de datos.\n",
    "dateToProcess = datetime.utcnow() ## Fecha Actual (Formato GMT)\n",
    "\n",
    "# Bloque de captura de datos desde WEB.\n",
    "\n",
    "# Imputamos una cabecera.\n",
    "head = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "# Capturamos el contenido desde la web y creamos un objeto tipo 'soup'.\n",
    "r = requests.get(web, headers=head, proxies=proxies)\n",
    "content = r.content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque PARSING del objeto SOUP.\n",
    "\n",
    "# Tras análisis de la estrucutra Capturamos todos las etiquetas 'td'\n",
    "td_all = soup.find_all('td')\n",
    "\n",
    "# Determinamos el largo de la tabla de forma dinámica por si cambia en el futuro\n",
    "country =(soup.findAll(\"a\", { \"class\" : \"mt_a\" }))\n",
    "\n",
    "paises = []\n",
    "for n in country:\n",
    "    aux = n.text\n",
    "    paises.append(aux)\n",
    "    \n",
    "paises = list(set(paises))\n",
    "numero_paises = len(paises)\n",
    "\n",
    "i=0\n",
    "k=12\n",
    "\n",
    "while i <= numero_paises:\n",
    "   \n",
    "    j= i*12\n",
    "    # Capturamos Pais\n",
    "    country = td_all[j+0+k].text\n",
    "    \n",
    "    # Capturamos total casos          \n",
    "    total_cases = td_all[j+1+k].text\n",
    "                \n",
    "    # Capturamos nuevos_casos\n",
    "    new_cases = (td_all[j+2+k].text)\n",
    "\n",
    "    # Capturamos muertes_totales\n",
    "    total_deaths = (td_all[j+3+k].text)\n",
    "    \n",
    "    # Capturamosnuevas_muertes\n",
    "    new_deaths = (td_all[j+4+k].text)\n",
    " \n",
    "    # Capturamos total recuperados\n",
    "    total_recovered = (td_all[j+5+k].text)\n",
    " \n",
    "    # Capturamos casos_activos\n",
    "    active_cases = (td_all[j+6+k].text)\n",
    " \n",
    "    # Capturamos criticos\n",
    "    critical_cases = (td_all[j+7+k].text)\n",
    " \n",
    "    # Capturamos casos_por millon\n",
    "    cases_per_million = (td_all[j+8+k].text)\n",
    "\n",
    "    # Capturamos muertes_por_millon\n",
    "    deaths_per_million = (td_all[j+9+k].text)\n",
    " \n",
    "    #append row to the dataframe\n",
    "\n",
    "    nueva_fila = {'dateTime':dateToProcess,'country':country, 'total_cases':total_cases,'new_cases':new_cases,\n",
    "            'total_deaths':total_deaths,'new_deaths':new_deaths,'total_recovered':total_recovered,\n",
    "            'active_cases':active_cases,'critical_cases':critical_cases,'cases_per_million':cases_per_million,\n",
    "            'deaths_per_million':deaths_per_million}\n",
    "    \n",
    "    #Añadimos una fila al dataset con los datos capturados\n",
    "\n",
    "    datos_pandemia_df = datos_pandemia_df.append(nueva_fila, ignore_index=True) ## DATAFRAME CON LA INFORMACIÓN RECOPILADA DESDE LA WEB WORLDOMETERS\n",
    "    \n",
    "    i= i+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
