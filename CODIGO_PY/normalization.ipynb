{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING PRACTICE - DATA NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # Librería para crear objetos tipo soup\n",
    "import urllib3 # Librería para descargar el contenido de una URL definida\n",
    "import numpy as np # Librería para manipular arreglos\n",
    "from sqlalchemy import create_engine # Librería para crear las conexiones con la base de datos Postgre\n",
    "from datetime import datetime\n",
    "from datetime import timedelta, time\n",
    "import pickle\n",
    "import requests\n",
    "import sys\n",
    "import re\n",
    "import random # Librería para generar números randómicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable para crear la conexión sobre el ambiente \"sandbox\"\n",
    "env = \"Sandbox\"\n",
    "frecuency = \"daily\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections for PostgreSQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../settings/hotDatamart{}.pkl'.format(env),'rb') as f:  \n",
    "    psqlUser,psqlPass,psqlHost,psqlPort,psqlDatabase = pickle.load(f)\n",
    "    \n",
    "psqlConnection = create_engine('postgresql://{}:{}@{}:{}/{}'.format(psqlUser,psqlPass,psqlHost,psqlPort,psqlDatabase),echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "##*********************** FUNCIONES PARA NORMALIZACIÓN DE DATOS DE AMAZON *********************\n",
    "\n",
    "# Función para efectuar la conversión de símbolo a moneda local de cada país\n",
    "def getCurrency(x): \n",
    "    if (\"€\" in x):\n",
    "        return (\"Euro\")\n",
    "    elif (\"$\" in x):\n",
    "        return (\"Dólar\")\n",
    "    else:\n",
    "        return (\"Libra Esterlina\")\n",
    "\n",
    "# Extracción del precio del campo \"price\"\n",
    "def getPrice(x): \n",
    "    # Aplicamos dos condiciones ya que el precio viene con coma o punto\n",
    "    if (\",\" in x):\n",
    "        temp = re.findall(r'\\d+\\,\\d+', x)\n",
    "        return temp\n",
    "    else:\n",
    "        temp = re.findall(r'\\d+\\.\\d+', x) \n",
    "        return temp\n",
    "\n",
    "# Reemplazo del punto por la coma donde corresponda  \n",
    "def replaceComma(x): \n",
    "    if (\",\" in x):\n",
    "        x = x.replace(',','.')\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "\n",
    "# Extracción del dato desde la lista\n",
    "def numConversion(x): \n",
    "    temp = x[0]\n",
    "    return temp\n",
    "\n",
    "##*********************** FUNCIONES PARA NORMALIZACIÓN DE DATOS DE CORONAVIRUS *********************\n",
    "\n",
    "# Función especial para automatizar la conversión de caracteres especiales y limpieza de datos de los datos sobre coronavirus \n",
    "def revisa_columna(x): \n",
    "    \n",
    "    if (x!=\"\" and x!=\" \" and x!=\"  \"): \n",
    "        x = x.strip()\n",
    "        x = x.replace(\"+\", \"\")\n",
    "        x = x.replace(\",\", \"\")\n",
    "        x = x.replace(\".\", \"\")\n",
    "        x = int(x)       \n",
    "                  \n",
    "    else:\n",
    "        x = 0\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAZON DATA NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "El dataframe obtenido de la extracción de datos de la web de Amazon \"dailyData\" pasa por el siguiente proceso de normalización de datos.\n",
    "\n",
    "El conjunto de datos contiene los siguientes campos normalizados:\n",
    "\n",
    "No hay N/A detectados\n",
    "\n",
    "'asin',              tipo CHR          \"Amazon Standar Identification Number - Código único de identificación del producto\"\n",
    "'description',       tipo CHR          \"Descripción del producto\"\n",
    "'dateTime'           tipo timestamp    \"Fecha y hora en la que se realiza la captura de datos\"\n",
    "'date'               tipo datetime     \"Fecha en la que se realiza la captura de datos\"\n",
    "'country'            tipo CHR          \"País al que pertenece el producto \"\n",
    "'currency'           tipo CHR          \"Moneda local en la que se expresa el precio del producto\"\n",
    "'price'              tipo FLOAT        \"Precio del producto\"\n",
    "\n",
    "Finalmente, se crean conexiones con una base PostgreSQL donde se los almacena durante el periodo de extracción definido.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalización de datos de AMAZON\n",
    "dailyData = dailyData.reset_index(drop=True)\n",
    "\n",
    "# Aplicamos la función getCurrency() para obtener la moneda local de cada país\n",
    "dailyData['currency'] = dailyData['price'].apply(getCurrency)\n",
    "\n",
    "# Obtenemos el precio que se encuentra dentro del string \"price\" y queda en una lista\n",
    "dailyData['price2'] = dailyData['price'].apply(getPrice)\n",
    "\n",
    "# Extraemos el precio de la lista y lo dejamos como string\n",
    "dailyData['price2'] = dailyData['price2'].apply(numConversion)\n",
    "\n",
    "# Reeplazamos el punto por la coma donde fuere necesario\n",
    "dailyData['price2'] = dailyData['price2'].apply(replaceComma)\n",
    "\n",
    "# Convertimos a formato flotante\n",
    "dailyData['price2'] = dailyData['price2'].astype(float)\n",
    "\n",
    "# Eliminamos el precio inicial y renombramos la columna procesada\n",
    "dailyData = dailyData.drop(['price'], axis=1, errors='ignore')\n",
    "dailyData = dailyData.reset_index(drop=True)\n",
    "dailyData = dailyData.rename(columns={'price2': 'price'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Amazon Data in postgre database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData.to_sql('{}_am_pr_detailed_final'.format(frecuency), con=psqlConnection, schema='platform' ,if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Amazon Data in .csv for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si el fichero ya está en el repositorio hacemos la lectura del mismo\n",
    "datos_disco_amazon = pd.read_csv(\"/home/jovyan/mylab/UOC/Respaldo/datos_amazon.csv\", sep=\";\")\n",
    "# Cremos un nuevo DF con la contatenación de lo que existía en el repositorio y lo generado en esta iteración.\n",
    "backup_stack = pd.concat([datos_disco_amazon, dailyData], axis=0)\n",
    "# Exportamos como un fichero CSV ACUMULADO los datos del DF con los nuevos ficheros añadidos\n",
    "backup_stack.to_csv('/home/jovyan/mylab/UOC/Respaldo/datos_amazon.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORONAVIRUS DATA NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nEl dataframe resultante de la extracción de datos de la web de Amazon \"dailyData\" pasa por el siguiente proceso de normalización de datos.\\n\\nFinalmente, se crean conexiones con una base PostgreSQL donde se los almacena durante el periodo de extracción definido.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "El dataframe obtenido de la extracción de datos de la web de Worldometers \"datos_pandemia_df\" pasa por el siguiente proceso de normalización de datos.\n",
    "\n",
    "El conjunto de datos contiene los siguientes campos normalizados:\n",
    "\n",
    "Datos de INT en unidades.\n",
    "\n",
    "No hay N/A detectados. Los valores no detectados son CERO.\n",
    "\n",
    "'dateTime'           tipo timestamp    \"Fecha y hora en la que se realiza la captura de datos\"\n",
    "'date'               tipo datetime     \"Fecha en la que se realiza la captura de datos\"\n",
    "'country',           tipo CHR          \"País al que hacen referencia los datos, salvo un crucero\"\n",
    "'total_cases'        tipo INT          \"Número total de casos que se han contabilizado en el país\"\n",
    "'new_cases'          tipo INT          \"Número total de casos incrementados desde la info anterior en el país\"\n",
    "'total_deaths'       tipo INT          \"Número total de fallidos en el país \"\n",
    "'new_deaths'         tipo INT          \"Número total de fallacidos incrementados desde la info anterior en el país\"\n",
    "'total_recovered'    tipo INT          \"Número total de recuperados en el país \"\n",
    "'active_cases'       tipo INT          \"Número total de casos en tratamiento en el país \"\n",
    "'critical_cases'     tipo INT          \"Número total en tratamiento en UCI en el país \"\n",
    "'cases_per_million'  tipo INT          \"Ratio de casos por millón de habitantes en el país \"\n",
    "'deaths_per_million' tipo INT          \"Ratio de muertes por millón de habitantes en el país \"\n",
    "\n",
    "Finalmente, se crean conexiones con una base PostgreSQL donde se los almacena durante el periodo de extracción definido.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coronavirus Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque de conversión de tipos y normalización de campos de DF que contiene los datos.\n",
    "\n",
    "# Creamos una funcion ad.hoc para cambiar el formato de las columnas a string\n",
    "datos_pandemia_df = datos_pandemia_df.replace(to_replace=r'', value=0, regex=True)\n",
    "datos_pandemia_df[['country','total_cases','new_cases','total_deaths','new_deaths','total_recovered','active_cases','critical_cases','cases_per_million','deaths_per_million']] = datos_pandemia_df[['country','total_cases','new_cases','total_deaths','new_deaths','total_recovered','active_cases','critical_cases','cases_per_million','deaths_per_million']].astype(str) \n",
    "\n",
    "  \n",
    "# Definimos las columnas que tienen que tener revisiones o cambios de formato.\n",
    "columnas2 = [ 'total_cases','new_cases','total_deaths','new_deaths','total_recovered','active_cases','critical_cases','cases_per_million','deaths_per_million']\n",
    "\n",
    "# Aplicamos la función anterior de limpieza a todo el DF de capturas.\n",
    "for n in columnas2:\n",
    "    datos_pandemia_df[n] = datos_pandemia_df[n].apply(revisa_columna)\n",
    "    \n",
    "datos_pandemia_df[\"date\"] = datos_pandemia_df[\"dateTime\"].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Coronavirus Data in postgre database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_pandemia_df.to_sql('{}_corona_detailed_final'.format(frecuency), con=psqlConnection, schema='platform' ,if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Amazon Data in .csv for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si el fichero ya está en el repositorio hacemos la lectura del mismo\n",
    "datos_disco_df = pd.read_csv(\"/home/jovyan/mylab/UOC/Respaldo/datos_pandemia.csv\", sep=\";\")\n",
    "# Cremos un nuevo DF con la contatenación de lo que existía en disco y lo generado en esta ejecución.\n",
    "vertical_stack = pd.concat([datos_disco_df, datos_pandemia_df], axis=0)\n",
    "# Exportamos como un fichero CSV ACUMULADO los datos del DF con los nuevos ficheros añadidos\n",
    "vertical_stack.to_csv('/home/jovyan/mylab/UOC/Respaldo/datos_pandemia.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKGROUND RESEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology used by the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'builtwith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8fd7509c4588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbuiltwith\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'builtwith'"
     ]
    }
   ],
   "source": [
    "import builtwith\n",
    "builtwith.parse('https://www.worldometers.info/coronavirus/')\n",
    "builtwith.parse('https://www.amazon.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owner of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whois\n",
    "print(whois.whois('https://www.amazon.com/'))\n",
    "print(whois.whois('https://www.worldometers.info/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotparser\n",
    "\n",
    "# Módulo robotparser para interpretar el fichero robots.txt de Amazon\n",
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url('https://www.amazon.com/robots.txt')\n",
    "rp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación de \"User Agent\"\n",
    "url = 'https://www.amazon.com'\n",
    "user_agent = 'BadCrawler' \n",
    "\n",
    "## Método can_fetch() para conocer si un \"user agent\" en particular tiene o no el acceso permitido a una página web\n",
    "rp.can_fetch(user_agent, url) \n",
    "user_agent = 'GoodCrawler' \n",
    "rp.can_fetch(user_agent, url) \n",
    "user_agent = 'Googlebot' \n",
    "rp.can_fetch(user_agent, url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
